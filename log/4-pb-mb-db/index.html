<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content=ekunazanu name=author><meta content="Designing a probabilistic database in under an MB, sort of." name=description><meta content="PB MB DB" property=og:title><meta content=article property=og:type><meta content="Designing a probabilistic database in under an MB, sort of." property=og:description><meta content=https://ekunazanu.foo/log/4-pb-mb-db/ property=og:url><meta content=https://ekunazanu.foo/thumbnails/log.4.pigeonhole.avif.png property=og:image><meta content="Doodle showing the pigeonhole principle." property=og:image:alt><meta content=image/png property=og:image:type><meta content=1200 property=og:image:width><meta content=900 property=og:image:height><meta content=en_US property=og:locale><meta content=ekunazanu.foo property=og:site_name><title>PB MB DB ~ ekunazanu.foo</title><link href=https://ekunazanu.foo/log/4-pb-mb-db/ rel=cannonical><link href=https://ekunazanu.foo/atom.xml rel=alternate type=application/atom+xml><link href=https://ekunazanu.foo/misc/main.css rel=stylesheet><link href=https://ekunazanu.foo/misc/favicon.png rel=icon><meta content=Zola name=generator><body><nav><ul><li><h2><a href=https://ekunazanu.foo>ekunazanu.foo</a></h2><li><a href=https://ekunazanu.foo/log>Log</a> ⟶<li><a href=https://ekunazanu.foo/lab>Lab</a></ul></nav><div class=print>https://ekunazanu.foo</div><main><article><h1>PB MB DB</h1><p>I didn’t get much work done this week; I was busy with my filmmaking classes. I watched movies all week <del>as part of my course</del>. I definitely wasn’t just procrastinating.<p>Okay, maybe I was.<p><img alt="doodle of a person saying tomorrow for a week" decoding=async loading=lazy src=/media/log/doodle-procrastinating.avif><p>But just because I did not code much does not mean I did not get anything done.<p>I created a design for a very simple database, for the next project. Well it is a database if you <em>really</em> stretch the definition of a database. Data can be added and queried, but it cannot be modified. This might seem like a very idiotic limitation, but I see it as precision engineering — removing (u)necessary features. So what is the purpose of this restrictive database? Storing counts for items. Possibly millions of items. While using just a single mebibyte. Okay, maybe a few bits over a mebibyte.<p>You’ll see why modifying counts is not really necessary (and perhaps, even a little disingenuous), once the project is up and running. And as for why should the size of the database be limited to a mebibyte? Well, why not? It makes the project a whole lot more interesting.<h2>Sketches</h2><p>Okay, a few things first. The only way I am getting this database down to a mebibyte is by using a sketch of the data instead of dealing with the actual data. This makes things a lot easier, but will cost precision. So much for precision engineering. Anyway, I am also pretty sure the data will have a highly skewed <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Zipf%27s_law target=_blank>Zipfian distribution</a>. So a <a href=/lab/probabilistic-data-structures#Count-Min_Sketch>count-min sketch</a> should do fine. Hopefully this priori holds, otherwise the precision will be abysmal.<p>I am also pretty sure that the count will easily exceed billions for the heavy hitters, so 32-bit counters won’t do, but 64-bits should be plenty enough. Now a count-min sketch guarantees that the estimate will exceed by ε|A| with probability at most δ — where |A| is the <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Norm_(mathematics) target=_blank>L1 norm</a> of the stream — for a sketch with width w = e/ε and depth d = ln(1/δ). That is, for a sketch with depth ln(1/δ) and width e/ε, the error will remain bounded by ε times the size of the input stream with a probability at least 1 - δ.<p>A count-min sketch of dimensions 32768 x 4 with 64-bit counters will use 32768 * 4 * 8 bytes — exactly a mebibyte. With d = 4, the probability of <strong>NOT</strong> overestimating the count by ε|A| is about 98%. Nice. However, ε|A| itself is huge as the norm of A is likely in the billions. But the input data stream is likely to follow a Zipfian distribution. Then the space required scales a bit differently — O(ε^(-1/z) * ln(1/δ)) — where z is the Zipf distribution parameter. Rearranging in terms of ε, the error scales by w^(-z) instead of e/w. So, even at z ≈ 2, the error bound decreases from a million to less than ten. Beautiful.<p>Now, multiplicity estimation uses exactly one mebibyte. Goal achieved. But as mentioned earlier, the database is slightly over a mebibyte. A few extra bits are reserved for some nice-to-haves — 2048B for cardinality estimation, 2048B for rank estimation and another 2048B to store the top 16 elements.<p>The cardinality estimation is done via <a href=/lab/probabilistic-data-structures#HyperLogLog>HyperLogLog</a> with 97.7% accuracy. The top items are stored on a min-heap, using the count-min sketch and principles from <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Misra%E2%80%93Gries_summary target=_blank>Misra–Gries summaries</a> for selection of elements. I am estimating ranks using a custom data structure loosely based on t-digest. Elements are selected at random, and their multiplicities are sorted and stored to create a simple distribution sketch. The rank is then calculated using cardinality estimates from HyperLogLog and by interpolating between the values from the distribution sketch. I am not good at math to prove guarantees for error bounds, but I will try tweaking it to get at least 98% accuracy.<p><img alt="a distribution sketch estimated using hyperloglog, count-min sketch and arrays of random estimates" decoding=async loading=lazy src=/media/log/hll-rank.avif><p>This brings the total size of the ‘database’ to 1054720 bytes — 1.01MB. Sweet. Can I add more features? Yes. Will feature creep set in? Very likely. So no features. It is not because I am lazy. I am very much not. Really.<p>Now, is it a real database? Well the transactions are atomic and consistent, and there is (complete) isolation and and durability. So perhaps? I don’t know, you decide. Either way, cya next week — hopefully by then, I will have actually built it.</article></main><footer><a href=https://ekunazanu.foo/more#Terms_of_Use>© 2025</a> <a href=https://ekunazanu.foo/about>ekunazanu</a> · <a rel="noopener nofollow noreferrer" href=https://creativecommons.org/licenses/by/4.0/ target=_blank>CC BY 4.0</a> · <a rel="noopener nofollow noreferrer" href=https://github.com/ekunazanu/ekunazanu.foo target=_blank>Source</a></footer>